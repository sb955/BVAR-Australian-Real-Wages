---
title: "Forecasting Australian Real Wages"
author: "Stephan Berke"

execute:
  echo: false
  
bibliography: references.bib
---

> **Abstract.** This project employs a Bayesian Vector Autoregression (BVAR) model to forecast Australian real wages for future periods. Beginning in 2021, Australian households experienced a shrinkage in real wages. However, recent trends suggest a recovery, leading to an expected upward trend in real wages in the coming periods.
>
> **Keywords.** BVARs, Real Wages, Inflation, Households

# Research Question

How will real wages and the broader economic conditions for households in Australia evolve in the upcoming periods?

# Motivation

Accurate forecasting of real wages is imperative for policymakers and businesses as it facilitates the anticipation of shifts in consumer purchasing power, which in turn substantially influences economic demand and inflationary dynamics. Such forecasts are integral to the formulation of monetary and fiscal policies aimed at maintaining economic stability and fostering long-term growth. The rising inflation and consequent escalation in the cost of living have imposed significant strain on Australian households (@RBA2024), compounded by the inability of nominal wage growth to sustain its historical average of approximately 4% since 2013 (@ABS2024WPI). This stagnation has resulted in a marked decline in real wages, particularly in the aftermath of the COVID-19 pandemic (@TreasuryAustralia2024). Notably, there has been a discernible upsurge in real wages since the last quarter. Nevertheless, historical projections by the Reserve Bank of Australia (RBA) have frequently failed to align with actual developments (@RBA2017), highlighting the necessity for a more refined predictive algorithm that can reliably anticipate future stable increases in real wages.

## Theory base

**Real Wages** defined as:

```{=tex}
\begin{align}
\log Real\:wages = \log Nominal\:wages\:- \log CPI 
\end{align}
```
# Data

Labor data is retrieved from the Australian Bureau of Statistics (ABS) using the **readabs** function and the Reserve Bank of Australia (RBA) using **readrba**.

The ABS releases data on average weekly earnings biannually, in May and December As the latest data is from 4 months ago, we will utilize the Wage Price Index (WPI), which is measured quarterly (last released in December 2023).

Key economic variables are incorporated to understand labor market and economic dynamics. Nominal and Real Wages assess purchasing power and income trends, with real wages adjusted using the Consumer Price Index (CPI) to account for inflation. Producer Prices, reflected by the Producer Price Index (PPI), provide insights into production costs and business environment.Export (EPI) and Import Indexes (IPI) are included to evaluate trade impacts and economic competitiveness. The Cost of Living Index (CLI) assesses consumer expenses, influencing economic welfare. Unemployment and Labor Participation Rates offer perspectives on labor market health and engagement. Log Hours Worked and Log Real GDP are used to analyze productivity trends and overall economic output.

## Indicators

#### Australian Labor and Financial Data

Following indicators will be used in the model. Indicators on global economic factors will be included later in this project.

| Indicator           | Index     | Source  | Unit        | Period    |
|---------------------|-----------|---------|-------------|-----------|
| Real Wages          | WPI - CPI | ABS/RBA | \%          | 1997-2024 |
| Inflation           | CPI       | ABS     | \%          | 1948-2023 |
| Producer Prices     | PPI       | ABS     | \%          | 1998-2023 |
| Export Index        | EPI       | ABS     | \%          | 1998-2023 |
| Import Index        | IPI       | ABS     | \%          | 1998-2023 |
| Cost of Living      | CLI       | ABS     | \%          | 1998-2023 |
| Unemployment        | UR        | RBA     | Persons     | 1978-2024 |
| Labor Participation | LPR       | ABS     | \%          | 1978-2024 |
| Log Hours Worked    | HW        | RBA     | \-          | 1978-2024 |
| Log Real GDP        | Real GDP  | RBA     | Million AUD | 1959-2023 |

## Data extraction and transformation

Since the main indicators are based on quarterly data, all indicators are converted into quarterly time series.

Given that the extracted time series data for the production price index starts in 1998 Q4, this quarter will work as the starting date of the analysis.

```{r libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(quantmod)
library(ggplot2)
library(readrba)
library(readabs)
library(dplyr)
library(xts)
library(tframePlus)
library(zoo)
library(knitr)
library(kableExtra)
library(forecast)
library(tseries)

```

```{r Colors , echo=FALSE,  message=FALSE, warning=FALSE, results='hide'}
# Define colors
mcxs1  = "#05386B"
mcxs2  = "#379683"
mcxs3  = "#5CDB95"
mcxs4  = "#8EE4AF"
mcxs5  = "#EDF5E1"

mcxs1.rgb   = col2rgb(mcxs1)
mcxs1.shade1= rgb(mcxs1.rgb[1],mcxs1.rgb[2],mcxs1.rgb[3], alpha=120, maxColorValue=255)
mcxs2.rgb   = col2rgb(mcxs2)
mcxs2.shade1= rgb(mcxs2.rgb[1],mcxs2.rgb[2],mcxs2.rgb[3], alpha=120, maxColorValue=255)
```

```{r data extraction, message=FALSE, warning=FALSE}

#define periods
start.period <- "1998-12-01"
start.period.qt <- "1998 Q4"
end.period <- "2023-12-31"
end.period.qt <- "2023 Q4"


# Get wage growth
log.nom.wpi <- log(subset(read_abs(series_id = "A2713851R"), date >= as.Date(start.period) & date <= as.Date(end.period))$value)
                          
# Get inflation rate (CPI)
log.cpi <- log(subset(read_abs(series_id = "A2325846C"), date >= as.Date(start.period) & date <= as.Date(end.period))$value)  
                          
# Define real wage
log.real.wpi <- log.nom.wpi - log.cpi

# Get dates
dates <- subset(read_abs(series_id = "A2713851R"), date >= as.Date(start.period) & date <= as.Date(end.period))$date
dates_yq <- as.yearqtr(dates)
dates_formatted <- format(dates_yq, "%Y Q%q")

# Get producer prices (PPI)
log.ppi <- log(subset(read_abs(series_id = "A2314865F"), date >= as.Date(start.period) & date <= as.Date(end.period))$value)

# Get export price index (EXPI)
log.expi <- log(subset(read_abs(series_id = "A2294886K"), date >= as.Date(start.period) & date <= as.Date(end.period))$value)

# Get import price index (IMPI)
log.impi <- log(subset(read_abs(series_id = "A2295765J"), date >= as.Date(start.period) & date <= as.Date(end.period))$value)

# Get cost of living index (CLI)
log.cli <- log(subset(read_abs(series_id = "A4083524T"), date >= as.Date(start.period) & date <= as.Date(end.period))$value)

# Get Log of Unemployment rate
log.unempl <- read_abs(series_id = "A84423092X") |> 
  select(date, value) |> 
  mutate(q_dates = as.yearqtr(date)) |> 
  filter(q_dates >= start.period.qt & q_dates <= end.period.qt) |> 
  group_by(q_dates) |> 
  summarise(value = mean(value, na.rm = TRUE)) |> 
  mutate(log_value = log(value)) |> 
  select(log_value)  |> 
  rename(log.unempl = log_value) |>
  select(log.unempl)

#Log of labor participation
log.parti <- read_abs(series_id = "A84423093A") |>
  select(date, value) |>
  mutate(q_dates = as.yearqtr(date),
    log_value = log(value)) |>
  filter(q_dates >= start.period.qt & q_dates <= end.period.qt) |>
  group_by(q_dates) |>
  summarise(mean_log_value = mean(log_value, na.rm = TRUE)) |>
  select(mean_log_value) |>
  rename(log.parti = mean_log_value)


#hours worked
log.h.worked <- read_rba_seriesid("GLFMHW") |>
  select(date, value) |>
  mutate(q_dates = as.yearqtr(date)) |>
  filter(q_dates >= start.period.qt & q_dates <= end.period.qt) |>
  group_by(q_dates) |>
  summarise(quarterly_sum = sum(value, na.rm = TRUE)) |>
  mutate(log_quarterly_sum = log(quarterly_sum)) |>
  select(log_quarterly_sum) |>
  rename(log.h.worked = log_quarterly_sum)

#real gdp
log.real.gdp <- log(subset(read_rba_seriesid("GGDPCVGDP"), date >= as.Date(start.period) & date <= as.Date(end.period))$value)

```

## Descriptive Analysis

### Data Overview

```{r HEAD, echo=FALSE}

# Create data frame
df <- data.frame(
  "Log of Real WPI" = log.real.wpi,
  "Log of CPI" = log.cpi,
  "Log of PPI" = log.ppi,
  "Log of EXPI" = log.expi,
  "Log of IMPI" = log.impi,
  "Log of CLI" = log.cli,
  "Log of Unemployment" = log.unempl,
  "Log of Participation" = log.parti,
  "Log Hours Worked" = log.h.worked,
  "Log of Real GDP" = log.real.gdp
)
# Display the first 5 rows of the dataframe
var_names <- colnames(df)
df_display <- data.frame(
  "Date" = as.Date(dates),
  df
)
kable(head(df_display, 5))
```

### Data Plots

The accompanying graphs depict the recent trends in the selected economic indicators. Notably, the Real Wage Price Index has dropped significantly since the beginning of COVID in March 2020. Concurrently, the Consumer Price Index has exhibited larger growth rates in the same period. Recently, a slight upward trend in Real Wages can be observed, though they remain below pre-pandemic levels.

Other indicators like Cost of Living Index, the Import Index and the Producer Price Index also reveal increasing trends, highlighting notable shifts in economic conditions, with Export Prices also shifting upwards. However, labor market conditions appear resilient, with falling unemployment, stable participation rates, and increased working hours. Real GDP's upward trend suggests a recovery to pre-pandemic economic activity levels.

```{r PLOTS, echo=FALSE}
# Plot data
par(mfrow = c(4, 3), mar = c(2, 2, 2, 2))

for (j in 1:ncol(df)) {
  plot(x = dates, y = df[, j], type = 'l',
       main = var_names[j], ylab = "", xlab = "",
       lwd = 2.5,
       col=mcxs2,
       ylim = c(min(df[, j], na.rm = TRUE), max(df[, j], na.rm = TRUE)))
}
```

## Preliminary data analysis

### ACF Analysis

The Autocorrelation Function (ACF) plots demonstrate a decay in correlation coefficients for all time series, with the memories of Import Index and Unemployment dropping slightly faster than the rest, indicating the presence of substantial memory. The initial lags exhibit significant autocorrelation, signifying non-stationary behavior. This persistence in the time series necessitates statistical differencing to ensure stationarity.

```{r ACF plots, echo = FALSE}
par(mfrow = c(4, 3), mar = c(2, 2, 3, 2))
for (j in 1:ncol(df)) {
  acf(df[,j], main=colnames(df[j]),  )
}
```

### PACF Analysis

The Partial Autocorrelation (PACF) plots for the time series predominantly exhibit significant partial autocorrelations at the first lag for all variables, followed by a rapid convergence to the confidence bounds. This pattern suggests that the data could be well represented by a first order autoregressive model.

```{r PACF plots, echo = FALSE}
par(mfrow = c(4, 3), mar = c(2, 2, 3, 2))
for (j in 1:ncol(df)) {
  pacf(df[,j], main=colnames(df)[j])
}
```

### Augmented Dickey-Fuller test

The ADF test results confirm the non-stationarity observed in the ACF plots. Specifically, first differences suffices for all variables except the unemployment rate, which is stationary on the 5% level, and the CPI and CLI, which require second-order differencingto in order to be stationary.

```{r ADF_table,  message=FALSE, warning=FALSE}

#code  idea from Yobin(2023)
perform_adf_tests <- function(df) {
  # Create an empty dataframe to store the results
  results <- data.frame(Variable = character(), TestType = character(), 
                        TestStatistic = numeric(), PValue = numeric(), 
                        stringsAsFactors = FALSE)

  # Iterate over each column in the dataframe
  for (col in colnames(df)) {
    # Remove NA values from the column
    column_data <- na.omit(df[[col]])

    # Perform ADF test for levels
    adf_levels <- tseries::adf.test(column_data, k = 4)

    # Check if p-value is less than or equal to 0.05
    if (adf_levels$p.value <= 0.05) {
      results <- bind_rows(results,
        data.frame(Variable = col, TestType = "Levels", 
                   TestStatistic = adf_levels$statistic,
                   PValue = adf_levels$p.value)
      )
    } else {
      # Perform ADF test for first difference
      adf_diff1 <- tseries::adf.test(diff(column_data), k = 4)
      
      # Check if p-value is less than 0.05
      if (adf_diff1$p.value < 0.05) {
        results <- bind_rows(results,
          data.frame(Variable = col, TestType = "First Difference", 
                     TestStatistic = adf_diff1$statistic,
                     PValue = adf_diff1$p.value)
        )
      } else {
        # Perform ADF test for second difference
        adf_diff2 <- tseries::adf.test(diff(column_data, differences = 2), k = 4)
        
        results <- bind_rows(results,
          data.frame(Variable = col, TestType = "Second Difference", 
                     TestStatistic = adf_diff2$statistic,
                     PValue = adf_diff2$p.value)
        )
      }
    }
  }

  # Return the results dataframe
  return(results)
}

adf_test_results <- perform_adf_tests(df)
rmarkdown::paged_table(adf_test_results, options = list(pageLength = 11))
```

# The baseline model

```{r  , echo=FALSE,  message=FALSE, warning=FALSE}
#new libraries
library(mvtnorm)
library(plot3D)
library(MASS)
library(HDInterval)
library(MCMCpack)
```

As an easy introduction, a bivariat random walk model will serve as a baseline model to apply the theorie.

```{r}
set.seed(9999)
rw_data = data.frame(matrix(nrow=100, ncol=2))
rw_data[,1] = cumsum(rnorm(100,0,1))
rw_data[,2] = cumsum(rnorm(100,0,1))
plot(rw_data[,1], type='l', ylim=c(min(rw_data), max(rw_data)), col=mcxs1, ylab='', xlab='', main='Bivariate Random Walk')
lines(rw_data[,2], col=mcxs2, ylab='', xlab='')
```

### VAR representation

To analyze the macroeconomic and financial variables and run forecasts, a Vector autoregression (VAR) model, introduced by @sims1972money can be applied, which allows for the multivariate framework of several time series. A general VAR model with $\rho$ lags for $t=1,...,T$ can be stated as:

```{=tex}
\begin{align}
y_{t}   &=\mu_{0}+A_{1}y_{t-1}+...+A_{\rho}y_{t-\rho}+\epsilon_{t}\\
\epsilon_{t}|Y_{T-1}    &\sim iid\left N_{N}(0_{N},\Sigma\right)
\end{align}
```
Where $y_{t}$ is a $N\times 1$ vector of observations at time $t$, $\mu_{0}$ is a $N\times1$ vector of constant terms, $A_{i}$ is a $N\times N$ matrix of the autoregressive slope parameters, $\epsilon_{t}$ is a $N \times 1$ vector of error terms, $Y_{t-1}$ is the information set and $\Sigma$ is the $N \times N$ covariance matrix of the error term.

### Baseline estimation

In order to define the baseline model, additional notation is being introduced following @wozniak2016bayesian.

Let a $K \times 1$ vector with $K=1+\rho N$ collect all of the data vectors on the RHS of the equation $\mathbf{x}_t = \left(1, y_{t-1}', \ldots, y_{t-\rho}' \right)$ and $K\times N$ matrix the intercept term and the autoregressive matrices $\mathbf{A} = \left(\mu, A_1, ... , A_{\rho} \right)'$, then the VAR process can be written as

```{=tex}
\begin{align}
y_t' &= x_t'A + \epsilon_t'.
\end{align}
```
If then all vectors $y_t'$ for t going from 1 to T are stacked under one another, one can form a $T \times N$ matrix $Y = (y_1, y_2, ..., y_T)'$ and similarly $X= (x_1, x_2,...,x_T)'$, with dimensions $T \times K$, as well as $E = (\epsilon_1, \epsilon_2,...,\epsilon_T)'$, a $T \times N$ matrix, to write the model as

```{=tex}
\begin{align}
Y=XA + E\\
Y|X,A,\Sigma &\sim MN_{T \times N} (XA, \Sigma, I_T).
\end{align}
```
```{r Base model set-up  , echo=TRUE, results='hide'}
y.bv <- ts(rw_data,  frequency=1)
N = ncol(y.bv)
p       = 1
K       = 1 + p*N
S       = 5000
h       = 8
Y.bv       = ts(y.bv[(p+1):nrow(y.bv),])
X.bv      = matrix(1,nrow(Y.bv),1)
for (i in 1:p){
  X.bv     = cbind(X.bv,y.bv[(p+1):nrow(y.bv)-i,])
}
```

The models likelihood function is

```{=tex}
\begin{align}
L(A,\Sigma|Y,X) &= (2\pi)^{-\frac{TN}{2}} det(\Sigma)^{-\frac{T}{2}}exp(-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)'(Y-XA)]),
\end{align}
```
which can be rewritten as

```{=tex}
\begin{align}
L(A,\Sigma|Y,X) &\propto det(\Sigma)^{-\frac{T}{2}} exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\hat{A})'X'X(A-\hat{A}) \right] \right\}\\
&\times exp \left\{-\frac{1}{2} tr \left[\Sigma^{-1}(Y-X \hat{A})'(Y-X \hat{A}) \right] \right\}.
\end{align}
```
The maximum likelihood estimators are represented by:

```{=tex}
\begin{align}
\hat{A} &= (X'X)^{-1}X'Y\\
\hat{\Sigma} &= \frac{1}{T} (Y-X \hat{A})'(Y-X \hat{A}).
\end{align}
```
```{r Maximum Likelihood  , echo=TRUE}
A.hat       = solve(t(X.bv)%*%X.bv)%*%t(X.bv)%*%Y.bv
Sigma.hat   = t(Y.bv-X.bv%*%A.hat)%*%(Y.bv-X.bv%*%A.hat)/nrow(Y.bv)
round(A.hat,3)
round(Sigma.hat,3)
#round(cov2cor(Sigma.hat),3)
```

The natural conjugate priors for $A$ and $\Sigma$ are assumed to follow a matrix normal and inverse Wishart distribution respectively:

```{=tex}
\begin{align}
p(A,\Sigma) &= MNIW(\underline{A},\underline{S},\underline{V},\underline{\nu})\\
A|\Sigma &\sim MN_{K \times N}(\underline{A},\Sigma,\underline{V}) \\
\Sigma &\sim IW_N(\underline{S},\underline{\nu}),
\end{align}
```
with the priors to be specified as Minnesota priors following @doan1984forecasting

```{=tex}
\begin{align}
\underline{A} &= [0_{N \times 1}, \quad I_N, \quad 0_{N \times (p-1)N}]'\\
\underline{V} &= diag([\kappa_2, \quad \kappa_1 (p^{-2} \otimes I_N)]).
\end{align}
```
```{r Minnesota priors, echo=TRUE}
N = ncol(Y.bv)
    
#set kapps
kappa.1 <- 0.02^2
kappa.2 <-100
K = 1 + (p*N)
  
A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N+1),] = diag(N)
V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior     = diag(diag(Sigma.hat))
nu.prior    = N+1

round(A.prior,3)
round(V.prior,3)
round(S.prior,3)
round(nu.prior,3)
```

The joint posterior distribution is then given by the product of the likelihood and the priors:

```{=tex}
\begin{align}
p(A,\Sigma|Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \\
&\times exp(-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)'(Y-XA)]) \\
&\times \det(\Sigma)^{-\frac{N+K+\underline{v}+1}{2}} \\
&\times exp(-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A})'\underline(V)^{-1}(A-\underline{A})]) \\
&\times exp(-\frac{1}{2}tr[\Sigma^{-1}\underline{S}]).
\end{align}
```
Combining terms yields the following distributions for $A$ and $\Sigma$:

```{=tex}
\begin{align}
p(A|Y,X,\Sigma) &= MN_{K \times N}(\bar{A}, \Sigma, \bar{V}) \\
p(\Sigma|Y,X) &= IW_N(\bar{S},\bar{\nu}),
\end{align}
```
where parameters $\bar{A}$ , $\bar{V}$, $\bar{S}$ and $\bar{\nu}$ characterising the posterior distribution and are given by

```{=tex}
\begin{align}
\bar{V} &= (X'X + \underline{V}^{-1})^{-1} \\
\bar{A} &= \bar{V}(X'Y + \underline{V}^{-1}\underline{A}) \\
\bar{\nu} &= T + \underline{\nu} \\
\bar{S} &= \underline{S} + Y'Y + \underline{A}'\underline{V}^{-1}\underline{A} - \bar{A}'\bar{V}^{-1}\bar{A}.
\end{align}
```
```{r NIW posterior paramters, echo=TRUE}
V.bar.inv   = t(X.bv)%*%X.bv + diag(1/diag(V.prior))
V.bar       = solve(V.bar.inv)
A.bar       = V.bar%*%(t(X.bv)%*%Y.bv + diag(1/diag(V.prior))%*%A.prior)
nu.bar      = nrow(Y.bv) + nu.prior
S.bar       = S.prior + t(Y.bv)%*%Y.bv + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
S.bar.inv   = solve(S.bar)

#round(A.bar,3)
#round(V.bar,3)
#round(S.bar,3)
#round(nu.bar,3)
```

### Posterior draws and Gibbs sampler

```{r Posterior darws , echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
Sigma.posterior   = apply(Sigma.posterior,3,solve)
Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
L                 = t(chol(V.bar))
for (s in 1:S){
  A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
}

#round(apply(A.posterior,1:2,mean),3)

# report posterior means and sd of parameters
A.E         = apply(A.posterior,1:2,mean)
A.sd        = apply(A.posterior,1:2,sd)
Sigma.E     = apply(Sigma.posterior,1:2,mean)
Sigma.sd    = apply(Sigma.posterior,1:2,sd)

round(A.E, 3)
```

```{r echo=TRUE}
round(Sigma.E, 3)
```

The estimated parameters are close to the expected values of 1 for a 1-period memory of a random process.

# Model extension: T-Distributed errors

An interesting specification in our model of forecasting real wages is the introduction of t-Distributed error terms. This specification is particularly notable because the t-distribution, with its heavier tails, may better account for potential outliers and the distributional characteristics of the time series data. This approach aims to enhance the model's robustness and accuracy in capturing the variability in real wage changes.

Following @chan2020large, the general Covariance structure is

```{=tex}
\begin{align}
vec(E) &\sim (0, \Sigma \otimes \Omega)
\end{align}
```
where $\Omega$ is a $T \times T$ covariance matrix.

If $\Omega=diag(\lambda_1,...,\lambda_T)$ and each $\lambda_t \mid \nu \sim \operatorname{IG2}(\nu, \nu)$, then $\epsilon_t$ has a multivariate t-distribution.

The likelihood is now given by

```{=tex}
\begin{align}
p(Y|A,\Sigma, \Omega) &= (2\pi)^{-\frac{TN}{2}} \det(\Omega)^{-\frac{T}{2}} det(\Sigma)^{-\frac{T}{2}} exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\hat{A})'X'X(A-\hat{A}) \right] \right\}\\
&\times exp \left\{-\frac{1}{2} tr \left[\Sigma^{-1}(Y-X \hat{A})'(Y-X \hat{A}) \right] \right\}.
\end{align}
```
In this framework, $A$ and $\Sigma$ will follow a normal-inverse-Wishart prior:

```{=tex}
\begin{align}
\Sigma \sim \operatorname{IW}(\underline{s}, \underline{v}), \quad (\operatorname{vec}(A) \mid \Sigma) \sim \mathcal{N}(\operatorname{vec}(\underline{A}), \Sigma \otimes \underline{V})
\end{align}
```
with joint density

```{=tex}
\begin{align}
p(A, \Sigma) &= \det(\Sigma)^{-\frac{\nu +N+K}{2}} \times exp\{-\frac{1}{2} tr[\Sigma^{-1}\underline{s}]\}  \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A})'\underline{V}^{-1}(A-\underline{A})]\}
\end{align}
```
The kernel of the joint posterior as our product of Likelihood and joint density is then expressed as

```{=tex}
\begin{align}
p(A,\Sigma|Y, \Omega) &\propto \det(\Sigma)^{-\frac{T}{2}} \times \det(\Omega)^{-\frac{N}{2}} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)' \Omega^{-1} (Y-XA)]\} \\
&\times \det(\Sigma)^{-\frac{N+K+\underline{v}+1}{2}} \times \det(\Sigma)^{-\frac{\nu +N+K}{2}}\\ 
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}\underline{s}]\}
\end{align}
```
After rearranging terms, the Kernel of the posterior distribution can be obtained as

```{=tex}
\begin{align}
&= \det(\Sigma)^{-\frac{N+K+\bar{v}+1}{2}} \times \det(\Omega)^{-\frac{N}{2}}\\
&\times exp\{-\frac{1}{2} tr[\Sigma^{-1}(A-\bar{A})'\bar{V}^{-1}(A-\bar{A})]\}\\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}\bar{s}]\},
\end{align}
```
which follows a multinomial-normal-inverse-wishart distribution:

```{=tex}
\begin{align}
p(A, \Sigma|Y,X) &\sim \operatorname{MNIW}(\bar{A}, \bar{V}, \bar{s}, \bar{v}).
\end{align}
```
The posterior parameters are then given as follows:

```{=tex}
\begin{align}
\bar{V} &= (X'\Omega^{-1}X + \underline{V}^{-1})^{-1} \\
\bar{A} &= \bar{V}(X'\Omega^{-1}Y + \underline{V}^{-1}\underline{A}) \\
\bar{\nu} &= T + \underline{\nu} \\
\bar{S} &= \underline{S} + Y'\Omega^{-1}Y + \underline{A}'\underline{V}^{-1}\underline{A} - \bar{A}'\bar{V}^{-1}\bar{A}.
\end{align}
```
### Distribution of $\lambda_t$

The likelihood defines as

```{=tex}
\begin{align}
L(A, \Sigma, \lambda | Y, X) = \lambda_t^{-\frac{N}{2}} \times \exp\left\{-\frac{1}{2} \frac{1}{\lambda_t} \epsilon_t' \Sigma^{-1} \epsilon_t \right\}.
\end{align}
```
The prior density of $\lambda \sim \operatorname{IG2}(\nu_{\lambda}, \nu_{\lambda})$ defined as

```{=tex}
\begin{align}
p(\lambda_t|\nu_{\lambda}) &= \lambda_t^{ -\frac{N+ \nu_{\lambda} +2}{2} } \times exp\{-\frac{1}{2} \frac{1}{\lambda_t} \nu_{\lambda}\}.
\end{align}
```
The joint posterior now derives in the common procedure

```{=tex}
\begin{align}
p(\lambda_t|Y,X,A,\Sigma) &\propto L(A,\Sigma,\lambda|Y,X) \times p(\lambda_t|\nu_{\lambda})\\
&= \lambda_t^{-\frac{N+ \nu_{\lambda} +2}{2}} \times exp\{-\frac{1}{2} \frac{1}{\lambda_t}(\nu_{\lambda} + \epsilon_t' \Sigma^{-1} \epsilon_t),
\end{align}
```
with $\epsilon_t = (y_t-x_t'A)$.

The kernel of the joint posterior follows an inverse gamma 2 distribution.

```{=tex}
\begin{align}
p(\lambda_t|Y,X,A,\Sigma) &\sim \operatorname{IG2}(N+\nu_{\lambda}, \nu_{\lambda}+\epsilon_t'\Sigma^{-1}\epsilon_t)
\end{align}
```
```{r echo=TRUE}
set.seed(9999)
rw_data = data.frame(matrix(nrow=100, ncol=2))
rw_data[,1] = cumsum(rnorm(100,0,1))
rw_data[,2] = cumsum(rnorm(100,0,1))
#plot(rw_data[,1], type='l', ylim=c(min(rw_data), max(rw_data)), col=mcxs1, ylab='', xlab='', main='Bivariate Random Walk')
#lines(rw_data[,2], col=mcxs, ylab='', xlab='')


y.bv <- ts(rw_data,  frequency=1)
N = ncol(y.bv)
p       = 1
S       = 100
h       = 8
Y.bv       = ts(y.bv[(p+1):nrow(y.bv),])
X.bv      = matrix(1,nrow(Y.bv),1)
for (i in 1:p){
  X.bv     = cbind(X.bv,y.bv[(p+1):nrow(y.bv)-i,])
}

A.hat       = solve(t(X.bv)%*%X.bv)%*%t(X.bv)%*%Y.bv
Sigma.hat   = t(Y.bv-X.bv%*%A.hat)%*%(Y.bv-X.bv%*%A.hat)/nrow(Y.bv)

t <- NROW(Y.bv)
kappa.1 <- 0.02^2
kappa.2 <-100
K = 1 + (p*N)

A.prior     = matrix(0, K , N)
A.prior[2:(N+1),] = diag(N)
V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior     = diag(diag(Sigma.hat))
nu.prior    = N+1
lambda.nu.prior = 5

#initialize
lambda.0 = rinvgamma(t, lambda.nu.prior/2, lambda.nu.prior/2)

Sigma.posterior.draws <- array(dim = c(N, N, S))
A.posterior.draws <- array(dim = c(K, N, S))
lambda.posterior.draws = array(NA,c(t,S+1))

for(s in 1:S){
  if (s == 1) {
    lambda.s = lambda.0
  } else {
    lambda.s    = lambda.posterior.draws[,s-1]
  }
  
  V.bar.ext       = solve(t(X.bv)%*%diag(1/(lambda.posterior.draws[,s]))%*%X.bv + solve(V.prior))
  A.bar.ext       = V.bar.ext%*%(t(X.bv)%*%diag(1/(lambda.posterior.draws[,s]))%*%Y.bv + solve(V.prior)%*%A.prior)
  nu.bar.ext      = t + nu.prior
  S.bar.ext       = S.prior + t(Y.bv)%*%diag(1/(lambda.posterior.draws[,s]))%*%Y.bv + t(A.prior)%*%solve(V.prior)%*%A.prior - t(A.bar.ext)%*%solve(V.bar.ext)%*%A.bar.ext
  #S.bar.ext.inv   = solve(S.bar.ext)
  #L               = t(chol(V.bar.ext.inv))
  
  Sigma.posterior.draws[,,s] = rWishart(1, nu.bar.ext, solve(S.bar.ext))[,,1]  #solve(S.bar.ext)??
  Sigma.posterior.draws[,,s] = solve(Sigma.posterior.draws[,,s])
  
  A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.bar.ext), sigma = Sigma.posterior.draws[,,s] %x% V.bar.ext), ncol=N)
  
  U_s <- Y.bv - X.bv %*% A.posterior.draws[,,s]
  
  #thanks to Yifang for nesting this out
  for (x in 1:t){
    lambda.s.posterior[,s] = lambda.nu.prior + t((U_s)[x,])%*%Sigma.posterior.draws[,,s]%*%(U_s)[x,]
  }
  
  lambda.v.posterior = N + lambda.nu.prior
  
  for (x in 1:t){
    lambda.posterior.draws[x,s+1] = rinvgamma(1,lambda.v.posterior/2, lambda.s.posterior[x,s]/2)
  }
} 
  

round(apply(A.posterior.draws[,,s], 1:2, mean),2)
round(apply(Sigma.posterior.draws[,,s], 1:2, mean),2) * round(mean(lambda.posterior.draws[,51:100]),2)
round(mean(lambda.posterior.draws[,s]),2)
```

One can observe, that the means represent exactly the expected values, whereas the variances lie a bit further apart from 1.

#### Include $\nu$ sampling

```{r echo=TRUE, }
#initialize chain
library(MCMCpack)
set.seed(9999)
rw_data = data.frame(matrix(nrow=100, ncol=2))
rw_data[,1] = cumsum(rnorm(100,0,1))
rw_data[,2] = cumsum(rnorm(100,0,1))


y.bv <- ts(rw_data,  frequency=1)
N = ncol(y.bv)
p       = 1
S       = 100
h       = 8
Y.bv       = ts(y.bv[(p+1):nrow(y.bv),])
X.bv      = matrix(1,nrow(Y.bv),1)
for (i in 1:p){
  X.bv     = cbind(X.bv,y.bv[(p+1):nrow(y.bv)-i,])
}

A.hat       = solve(t(X.bv)%*%X.bv)%*%t(X.bv)%*%Y.bv
Sigma.hat   = t(Y.bv-X.bv%*%A.hat)%*%(Y.bv-X.bv%*%A.hat)/nrow(Y.bv)

t <- NROW(Y.bv)
kappa.1 <- 0.02^2
kappa.2 <-100
K = 1 + (p*N)
nu=5

A.prior     = matrix(0, K , N)
A.prior[2:(N+1),] = diag(N)
V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior     = diag(diag(Sigma.hat))
nu.prior    = N+1
lambda.nu.prior = 3.2


#initialize chain
lambda.init = rinvgamma(t, lambda.nu.prior/2, lambda.nu.prior/2)
omega.init = diag(lambda.init)
omega.init.inv = solve(omega.init)


Sigma.posterior.draws <- array(dim = c(N, N, S))
A.posterior.draws <- array(dim = c(K, N, S))
Omega.posterior.draws <- array(dim=c(t,t,S+1))
Omega.posterior.draws[,,1] <- omega.init

#store_nu <- numeric(S)
lambda.posterior.draws = array(NA,c(t,S))
lambda.s.posterior = array(NA,c(t,S))

#nudraws
nu.draws = array(NA,S)
nuub = 30
countnu = 0

for(s in 1:S){
  Omega.inv_s     = solve(Omega.posterior.draws[,,s])
  
  V.bar.ext       = solve(t(X.bv)%*%Omega.inv_s%*%X.bv + solve(V.prior))
  A.bar.ext       = V.bar.ext%*%(t(X.bv)%*%Omega.inv_s%*%Y.bv + solve(V.prior)%*%A.prior)
  nu.bar.ext      = t + nu.prior
  S.bar.ext       = S.prior + t(Y.bv)%*%Omega.inv_s%*%Y.bv + t(A.prior)%*%solve(V.prior)%*%A.prior - t(A.bar.ext)%*%solve(V.bar.ext)%*%A.bar.ext
  S.bar.ext       = (S.bar.ext %*% t(S.bar.ext))/2
  #S.bar.ext.inv   = solve(S.bar.ext)
  #L               = t(chol(V.bar.ext.inv))
  
  Sigma.posterior.draws[,,s] = rWishart(1, nu.bar.ext, solve(S.bar.ext))[,,1]  #solve(S.bar.ext)??
  Sigma.posterior.draws[,,s] = solve(Sigma.posterior.draws[,,s])
  A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.bar.ext), sigma = Sigma.posterior.draws[,,s] %x% V.bar.ext), ncol=N)
  
  U_s <- Y.bv - X.bv %*% A.posterior.draws[,,s]
  for (x in 1:t){
    lambda.s.posterior[,s] = lambda.nu.prior + t((U_s)[x,])%*%Sigma.posterior.draws[,,s]%*%(U_s)[x,]
  }
  
  lambda.v.posterior = N + lambda.nu.prior
  
  for (x in 1:t){
    lambda.posterior.draws[x,s] = rinvgamma(1,lambda.v.posterior/2, lambda.s.posterior[x,s]/2)
  }
  
  Omega.posterior.draws[,,s+1] = diag(lambda.posterior.draws[,s])
  
  
  binary <- 1
  sum1 <- sum(log(lambda.posterior.draws[,s]))
  sum2 <- sum(1/lambda.posterior.draws[,s])
  
  funct.nu <- function(x) {
    t * (x/2 * log(x/2) - lgamma(x/2)) - (x/2 + 1) * sum1 - x/2 * sum2
  }
  #first derivative
  f1 <- function(x) {
    t/2 * (log(x/2) + 1 - digamma(x/2)) - 0.5 * (sum1 + sum2)
  }
  #second derivative
  f2 <- function(x) {
    t/(2*x) - t/4 * trigamma(x/2)
  }
  
  err_nu <- 1
  nut <- lambda.nu.prior
  while (abs(err_nu) > 10^(-5)) {
    err_nu <- f1(nut)
    Knu <- -f2(nut)
    nut <- nut + err_nu / Knu
    if (nut < 2) {
      nut <- 5
      Knu <- -f2(nut)
      break
    }
  }
  
  sqrtDnu <- sqrt(1/Knu)
  nuc <- rnorm(1, mean = nut, sd = sqrtDnu)
  
  if (nuc > 2 && nuc < nuub) {
    alp_MH <- exp(funct.nu(nuc) - funct.nu(lambda.nu.prior)) * dnorm(lambda.nu.prior, mean = nut, sd = sqrtDnu) / dnorm(nuc, mean = nut, sd = sqrtDnu)
    if (alp_MH > runif(1)) {
      nu.draws[s] <- nuc
      binary <- 1
    }
  }
  list(lambda.nu.prior = lambda.nu.prior, binary = binary, funct.nu = funct.nu)
} 


round(apply(A.posterior.draws[,,s], 1:2, mean),2)
round(apply(Sigma.posterior.draws[,,s], 1:2, mean),2) * round(mean(lambda.posterior.draws[,51:100]),2)
round(mean(lambda.posterior.draws[,s]),2)
round(mean(nu.draws[s]),2)

```

The results vary a lot when adjusting the prior of $\lambda_{nu}$.

#### Further research

Running the code with 100 samples is already very computational intensive. I will be necessary to apply tools by @chan2020large to reduce computational intensity.

A grid search search for the prior of $\lambda_{\nu}$, since the models parameters explode for guesses outside a specific range.

### Real Wage data forecasts

```{r Model set-up  , echo=FALSE, results='hide'}
y <- ts(df, start=c(1998, 4), frequency=4)
N = ncol(y)
p       = 4
K       = 1 + p*N
S       = 5000
h       = 8
Y       = ts(y[(p+1):nrow(y),])
X       = matrix(1,nrow(Y),1)
for (i in 1:p){
  X     = cbind(X,y[(p+1):nrow(y)-i,])
}

A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
round(A.hat,3)
round(Sigma.hat,3)
round(cov2cor(Sigma.hat),3)
N = ncol(Y)
#set kapps
kappa.1 <- 0.02^2
kappa.2 <-100
K = 1 + (p*N)

A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N+1),] = diag(N)
V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior     = diag(diag(Sigma.hat))
nu.prior    = N+1

# normal-inverse Wishart posterior parameters
V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))
V.bar       = solve(V.bar.inv)
A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
nu.bar      = nrow(Y) + nu.prior
S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
S.bar.inv   = solve(S.bar)

# posterior draws 
Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
Sigma.posterior   = apply(Sigma.posterior,3,solve)
Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
L                 = t(chol(V.bar))
for (s in 1:S){
  A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
}

### Simulate draws from predictive density
Y.h         = array(NA,c(h,N,S))

for (s in 1:S){
  x.Ti        = Y[(nrow(Y)-h+1):nrow(Y),]
  x.Ti        = x.Ti[4:1,]
  for (i in 1:h){
    x.T         = c(1,as.vector(t(x.Ti)))
    Y.h[i,,s]   = rmvnorm(1, mean = x.T%*%A.posterior[,,s], sigma=Sigma.posterior[,,s])
    x.Ti        = rbind(Y.h[i,,s],x.Ti[1:3,])
  }
}

```

```{r Forecast plots}

x_dates <- format(as.yearqtr(c(dates, seq(tail(dates, 1), by = "quarter", length.out = h + 1)[-1])), "%Y Q%q")
par(mfrow = c(5, 2), mar = c(2, 2, 1, 1), oma = c(0, 0, 2, 0))

for (i in 1:10) {
    point.f <- apply(Y.h[, i, ], 1, mean)
    interval.f <- apply(Y.h[, i, ], 1, hdi, credMass = 0.90)
    combined_data <- c(y[, i], point.f)
    range_val <- range(combined_data, interval.f)

    plot(1:(nrow(y) + h), combined_data, type = "l", ylim = range_val, xlab = "", ylab = "", col = mcxs2, lwd = 2, main = paste("Forecast", var_names[i]), bty = "n", xaxt = "n")
    axis(1, at = 1:(nrow(y) + h), labels= x_dates, cex.axis = 0.7, tck = 0)
    abline(v = nrow(y)+1, col = mcxs1)  
    
    polygon(c((nrow(y)+1):(nrow(y)+h), rev((nrow(y)+1):(nrow(y)+h))),
            c(interval.f[1, ], rev(interval.f[2, ])), col = mcxs2.shade1, border = mcxs1.shade1)
}
```

We observe the real wages to stabilize in the future, but not to reach levels of the pre-Covid area.

# References {.unnumbered}
