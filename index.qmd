---
title: "Forecasting Australian Real Wages"
author: "Stephan Berke"

execute:
  echo: false
  
bibliography: references.bib
---

> **Abstract.** This project employs a Bayesian Vector Autoregression (BVAR) model to forecast Australian real wages for future periods. Beginning in 2021, Australian households experienced a shrinkage in real wages. However, recent trends suggest a recovery, leading to an expected upward trend in real wages in the coming periods.
>
> **Keywords.** BVARs, Real Wages, Inflation, Households

# Research Question

How will real wages and the broader economic conditions for households in Australia evolve in the upcoming periods?

# Motivation

Accurate forecasting of real wages is imperative for policymakers and businesses as it facilitates the anticipation of shifts in consumer purchasing power, which in turn substantially influences economic demand and inflationary dynamics. Such forecasts are integral to the formulation of monetary and fiscal policies aimed at maintaining economic stability and fostering long-term growth. The rising inflation and consequent escalation in the cost of living have imposed significant strain on Australian households (@RBA2024), compounded by the inability of nominal wage growth to sustain its historical average of approximately 4% since 2013 (@ABS2024WPI). This stagnation has resulted in a marked decline in real wages, particularly in the aftermath of the COVID-19 pandemic (@TreasuryAustralia2024). Notably, there has been a discernible upsurge in real wages since the last quarter. Nevertheless, historical projections by the Reserve Bank of Australia (RBA) have frequently failed to align with actual developments (@RBA2017), highlighting the necessity for a more refined predictive algorithm that can reliably anticipate future stable increases in real wages.

## Theory base

**Real Wages** defined as:

$$
\log Real\:wages = \log Nominal\:wages\:- \log CPI 
$$
# Data

Labor data is retrieved from the Australian Bureau of Statistics (ABS) using the **readabs** function and the Reserve Bank of Australia (RBA) using **readrba**.

The ABS releases data on average weekly earnings biannually, in May and December As the latest data is from 4 months ago, we will utilize the Wage Price Index (WPI), which is measured quarterly (last released in December 2023).

Key economic variables are incorporated to understand labor market and economic dynamics. Nominal and Real Wages assess purchasing power and income trends, with real wages adjusted using the Consumer Price Index (CPI) to account for inflation. Producer Prices, reflected by the Producer Price Index (PPI), provide insights into production costs and business environment.Export (EPI) and Import Indexes (IPI) are included to evaluate trade impacts and economic competitiveness. The Cost of Living Index (CLI) assesses consumer expenses, influencing economic welfare. Unemployment and Labor Participation Rates offer perspectives on labor market health and engagement. Log Hours Worked and Log Real GDP are used to analyze productivity trends and overall economic output.

## Indicators

#### Australian Labor and Financial Data

Following indicators will be used in the model. Indicators on global economic factors will be included later in this project.

| Indicator           | Index     | Source  | Unit        | Period    |
|---------------------|-----------|---------|-------------|-----------|
| Real Wages          | WPI - CPI | ABS/RBA | \%          | 1997-2024 |
| Inflation           | CPI       | ABS     | \%          | 1948-2023 |
| Producer Prices     | PPI       | ABS     | \%          | 1998-2023 |
| Export Index        | EPI       | ABS     | \%          | 1998-2023 |
| Import Index        | IPI       | ABS     | \%          | 1998-2023 |
| Cost of Living      | CLI       | ABS     | \%          | 1998-2023 |
| Unemployment        | UR        | RBA     | Persons     | 1978-2024 |
| Labor Participation | LPR       | ABS     | \%          | 1978-2024 |
| Log Hours Worked    | HW        | RBA     | \-          | 1978-2024 |
| Log Real GDP        | Real GDP  | RBA     | Million AUD | 1959-2023 |

## Data extraction and transformation

Since the main indicators are based on quarterly data, all indicators are converted into quarterly time series.

Given that the extracted time series data for the production price index starts in 1998 Q4, this quarter will work as the starting date of the analysis.

```{r libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(quantmod)
library(ggplot2)
library(readrba)
library(readabs)
library(dplyr)
library(xts)
library(tframePlus)
library(zoo)
library(knitr)
library(kableExtra)
library(forecast)
library(tseries)

```

```{r Colors , echo=FALSE,  message=FALSE, warning=FALSE, results='hide'}
# Define colors
mcxs1  = "#05386B"
mcxs2  = "#379683"
mcxs3  = "#5CDB95"
mcxs4  = "#8EE4AF"
mcxs5  = "#EDF5E1"

mcxs1.rgb   = col2rgb(mcxs1)
mcxs1.shade1= rgb(mcxs1.rgb[1],mcxs1.rgb[2],mcxs1.rgb[3], alpha=120, maxColorValue=255)
mcxs2.rgb   = col2rgb(mcxs2)
mcxs2.shade1= rgb(mcxs2.rgb[1],mcxs2.rgb[2],mcxs2.rgb[3], alpha=120, maxColorValue=255)
```

```{r data extraction, message=FALSE, warning=FALSE}

#define periods
start.period <- "1998-12-01"
start.period.qt <- "1998 Q4"
end.period <- "2024-03-01"
end.period.qt <- "2024 Q1"


# Get wage growth
log.nom.wpi <- log(subset(read_abs(series_id = "A2713851R"), date >= as.Date(start.period) & date <= as.Date(end.period))$value)
                          
# Get inflation rate (CPI)
log.cpi <- log(subset(read_abs(series_id = "A2325846C"), date >= as.Date(start.period) & date <= as.Date(end.period))$value)  
                          
# Define real wage
log.real.wpi <- log.nom.wpi - log.cpi

# Get dates
dates <- subset(read_abs(series_id = "A2713851R"), date >= as.Date(start.period) & date <= as.Date(end.period))$date
dates_yq <- as.yearqtr(dates)
dates_formatted <- format(dates_yq, "%Y Q%q")

# Get producer prices (PPI)
log.ppi <- log(subset(read_abs(series_id = "A2314865F"), date >= as.Date(start.period) & date <= as.Date(end.period))$value)

# Get export price index (EXPI)
log.expi <- log(subset(read_abs(series_id = "A2294886K"), date >= as.Date(start.period) & date <= as.Date(end.period))$value)

# Get import price index (IMPI)
log.impi <- log(subset(read_abs(series_id = "A2295765J"), date >= as.Date(start.period) & date <= as.Date(end.period))$value)

# Get cost of living index (CLI)
log.cli <- log(subset(read_abs(series_id = "A4083524T"), date >= as.Date(start.period) & date <= as.Date(end.period))$value)

#real gdp
log.real.gdp <- log(subset(read_rba_seriesid("GGDPCVGDP"), date >= as.Date("1998-12-07") & date <= as.Date("2024-03-07"))$value)

# Get Log of Unemployment rate
log.unempl <- read_abs(series_id = "A84423092X") |> 
  select(date, value) |> 
  mutate(q_dates = as.yearqtr(date)) |> 
  filter(q_dates >= start.period.qt & q_dates <= end.period.qt) |> 
  group_by(q_dates) |> 
  summarise(value = mean(value, na.rm = TRUE)) |> 
  mutate(log_value = log(value)) |> 
  select(log_value)  |> 
  rename(log.unempl = log_value) |>
  select(log.unempl)

#Log of labor participation
log.parti <- read_abs(series_id = "A84423093A") |>
  select(date, value) |>
  mutate(q_dates = as.yearqtr(date),
    log_value = log(value)) |>
  filter(q_dates >= start.period.qt & q_dates <= end.period.qt) |>
  group_by(q_dates) |>
  summarise(mean_log_value = mean(log_value, na.rm = TRUE)) |>
  select(mean_log_value) |>
  rename(log.parti = mean_log_value)


#hours worked
log.h.worked <- read_rba_seriesid("GLFMHW") |>
  select(date, value) |>
  mutate(q_dates = as.yearqtr(date)) |>
  filter(q_dates >= start.period.qt & q_dates <= end.period.qt) |>
  group_by(q_dates) |>
  summarise(quarterly_sum = sum(value, na.rm = TRUE)) |>
  mutate(log_quarterly_sum = log(quarterly_sum)) |>
  select(log_quarterly_sum) |>
  rename(log.h.worked = log_quarterly_sum)

```

## Descriptive Analysis

### Data Overview

```{r HEAD, echo=FALSE}

# Create data frame
df <- data.frame(
  "Log of Real WPI" = log.real.wpi,
  "Log of CPI" = log.cpi,
  "Log of PPI" = log.ppi,
  "Log of EXPI" = log.expi,
  "Log of IMPI" = log.impi,
  "Log of CLI" = log.cli,
  #"Log of Real GDP" = log.real.gdp,
  "Log of Unemployment" = log.unempl,
  "Log of Participation" = log.parti,
  "Log Hours Worked" = log.h.worked
)
# Display the first 5 rows of the dataframe
var_names <- colnames(df)
df_display <- data.frame(
  "Date" = as.Date(dates),
  df
)
kable(head(df_display, 5))
```

### Data Plots

The accompanying graphs depict the recent trends in the selected economic indicators. Notably, the Real Wage Price Index has dropped significantly since the beginning of COVID in March 2020. Concurrently, the Consumer Price Index has exhibited larger growth rates in the same period. Recently, a slight upward trend in Real Wages can be observed, though they remain below pre-pandemic levels.

Other indicators like Cost of Living Index, the Import Index and the Producer Price Index also reveal increasing trends, highlighting notable shifts in economic conditions, with Export Prices also shifting upwards. However, labor market conditions appear resilient, with falling unemployment, stable participation rates, and increased working hours. Real GDP's upward trend suggests a recovery to pre-pandemic economic activity levels.

```{r PLOTS, echo=FALSE}
# Plot data
#par(mfrow = c(3, 3), mar = c(2, 2, 2, 2))

#original_par <- par()
#par(original_par)
data.plots <- function(data, var_names){
  par(mfrow = c(3, 3), mar = c(2, 2, 2, 2))
  for (j in 2:ncol(data)) 
  plot(x = data$Date, y = data[, j], type = 'l',
       main = var_names[j-1], ylab = "", xlab = "",
       lwd = 2.5,
       col=mcxs2,
       ylim = c(min(data[, j], na.rm = TRUE), max(data[, j], na.rm = TRUE)))
}
data.plots(df_display, var_names)
```

## Preliminary data analysis

### ACF Analysis

The Autocorrelation Function (ACF) plots demonstrate a decay in correlation coefficients for all time series, with the memories of Import Index and Unemployment dropping slightly faster than the rest, indicating the presence of substantial memory. The initial lags exhibit significant autocorrelation, signifying non-stationary behavior. This persistence in the time series necessitates statistical differencing to ensure stationarity.

```{r ACF plots, echo = FALSE}
par(mfrow = c(4, 3), mar = c(2, 2, 3, 2))
for (j in 1:ncol(df)) {
  acf(df[,j], main=colnames(df[j]),  )
}
```

### PACF Analysis

The Partial Autocorrelation (PACF) plots for the time series predominantly exhibit significant partial autocorrelations at the first lag for all variables, followed by a rapid convergence to the confidence bounds. This pattern suggests that the data could be well represented by a first order autoregressive model.

```{r PACF plots, echo = FALSE}
par(mfrow = c(4, 3), mar = c(2, 2, 3, 2))
for (j in 1:ncol(df)) {
  pacf(df[,j], main=colnames(df)[j])
}
```

### Augmented Dickey-Fuller test

The ADF test results confirm the non-stationarity observed in the ACF plots. Specifically, first differences suffices for all variables except the unemployment rate, which is stationary on the 5% level, and the CPI and CLI, which require second-order differencing in order to be stationary.

```{r ADF_table,  message=FALSE, warning=FALSE}

#code  idea from Yobin(2023)
perform_adf_tests <- function(df) {
  # Create an empty dataframe to store the results
  results <- data.frame(Variable = character(), TestType = character(), 
                        TestStatistic = numeric(), PValue = numeric(), 
                        stringsAsFactors = FALSE)

  # Iterate over each column in the dataframe
  for (col in colnames(df)) {
    # Remove NA values from the column
    column_data <- na.omit(df[[col]])

    # Perform ADF test for levels
    adf_levels <- tseries::adf.test(column_data, k = 4)

    # Check if p-value is less than or equal to 0.05
    if (adf_levels$p.value <= 0.05) {
      results <- bind_rows(results,
        data.frame(Variable = col, TestType = "Levels", 
                   TestStatistic = adf_levels$statistic,
                   PValue = adf_levels$p.value)
      )
    } else {
      # Perform ADF test for first difference
      adf_diff1 <- tseries::adf.test(diff(column_data), k = 4)
      
      # Check if p-value is less than 0.05
      if (adf_diff1$p.value < 0.05) {
        results <- bind_rows(results,
          data.frame(Variable = col, TestType = "First Difference", 
                     TestStatistic = adf_diff1$statistic,
                     PValue = adf_diff1$p.value)
        )
      } else {
        # Perform ADF test for second difference
        adf_diff2 <- tseries::adf.test(diff(column_data, differences = 2), k = 4)
        
        results <- bind_rows(results,
          data.frame(Variable = col, TestType = "Second Difference", 
                     TestStatistic = adf_diff2$statistic,
                     PValue = adf_diff2$p.value)
        )
      }
    }
  }
  return(results)
}

adf_test_results <- perform_adf_tests(df)
rmarkdown::paged_table(adf_test_results, options = list(pageLength = 11))
```

# The baseline model

```{r  , echo=FALSE,  message=FALSE, warning=FALSE}
#new libraries
library(mvtnorm)
library(plot3D)
library(MASS)
library(HDInterval)
library(MCMCpack)
```

To provide a straightforward introduction, we will utilize a bivariate random walk model as a baseline. This approach will facilitate a clearer comprehension of the underlying theory.

```{r}
set.seed(234786)
rw_data = data.frame(matrix(nrow=1000, ncol=2))
rw_data[,1] = cumsum(rnorm(1000,0,1))
rw_data[,2] = cumsum(rnorm(1000,0,1))
plot(rw_data[,1], type='l', ylim=c(min(rw_data), max(rw_data)), col=mcxs1, ylab='', xlab='', main='Bivariate Random Walk')
lines(rw_data[,2], col=mcxs2, ylab='', xlab='')
```

### VAR representation

To analyze the macroeconomic and financial variables and run forecasts, a Vector autoregression (VAR) model, introduced by @sims1972money can be applied, which allows for the multivariate framework of several time series. A general VAR model with $\rho$ lags for $t=1,...,T$ can be stated as:

```{=tex}
\begin{align}
y_{t} = \mu_{0} + A_{1} y_{t-1} + \ldots + A_{\rho} y_{t-\rho} + \epsilon_{t}\\
\epsilon_{t} \mid Y_{t-1} \sim \text{i.i.d. } \mathcal{N}_{N}(0_{N}, \Sigma)
\end{align}
```

Where $y_{t}$ is a $N\times 1$ vector of observations at time $t$, $\mu_{0}$ is a $N\times1$ vector of constant terms, $A_{i}$ is a $N\times N$ matrix of the autoregressive slope parameters, $\epsilon_{t}$ is a $N \times 1$ vector of error terms, $Y_{t-1}$ is the information set and $\Sigma$ is the $N \times N$ covariance matrix of the error term.

### Baseline estimation

In order to define the baseline model, additional notation is being introduced following @wozniak2016bayesian.

Let a $K \times 1$ vector with $K=1+\rho N$ collect all of the data vectors on the RHS of the equation $\mathbf{x}_t = \left(1, y_{t-1}', \ldots, y_{t-\rho}' \right)$ and $K\times N$ matrix the intercept term and the autoregressive matrices $\mathbf{A} = \left(\mu, A_1, ... , A_{\rho} \right)'$, then the VAR process can be written as

$$
y_t' = x_t'A + \epsilon_t'.
$$
If then all vectors $y_t'$ for t going from 1 to T are stacked under one another, one can form a $T \times N$ matrix $Y = (y_1, y_2, ..., y_T)'$ and similarly $X= (x_1, x_2,...,x_T)'$, with dimensions $T \times K$, as well as $E = (\epsilon_1, \epsilon_2,...,\epsilon_T)'$, a $T \times N$ matrix, to write the model as

```{=tex}
\begin{align}
Y=XA + E\\
Y|X,A,\Sigma \sim MN_{T \times N} (XA, \Sigma, I_T).
\end{align}
```

```{r Base model set-up  , echo=TRUE, results='hide'}
y.bv <- ts(rw_data,  frequency=1)
N = ncol(y.bv)
p       = 1
K       = 1 + p*N
S       = 1000
Y.bv       = ts(y.bv[(p+1):nrow(y.bv),])
X.bv      = matrix(1,nrow(Y.bv),1)
for (i in 1:p){
  X.bv     = cbind(X.bv,y.bv[(p+1):nrow(y.bv)-i,])
}
```

The models likelihood function is

```{=tex}
\begin{align}
L(A,\Sigma|Y,X) &= (2\pi)^{-\frac{TN}{2}}  det(\Sigma)^{-\frac{T}{2}}exp\{-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)'(Y-XA)]\},
\end{align}
```

which can be rewritten as

```{=tex}
\begin{align}
L(A,\Sigma|Y,X) \propto det(\Sigma)^{-\frac{T}{2}} exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\hat{A})'X'X(A-\hat{A}) \right] \right\}\\
\times exp \left\{-\frac{1}{2} tr \left[\Sigma^{-1}(Y-X \hat{A})'(Y-X \hat{A}) \right] \right\}.
\end{align}
```

The maximum likelihood estimators are represented by:

```{=tex}
\begin{align}
\hat{A} &= (X'X)^{-1}X'Y\\
\hat{\Sigma} &= \frac{1}{T} (Y-X \hat{A})'(Y-X \hat{A}).
\end{align}
```

```{r Maximum Likelihood  , echo=TRUE}
A.hat       = solve(t(X.bv)%*%X.bv)%*%t(X.bv)%*%Y.bv
Sigma.hat   = t(Y.bv-X.bv%*%A.hat)%*%(Y.bv-X.bv%*%A.hat)/nrow(Y.bv)
round(A.hat,3)
round(Sigma.hat,3)
#round(cov2cor(Sigma.hat),3)
```

The natural conjugate priors for $A$ and $\Sigma$ are assumed to follow a matrix normal and inverse Wishart distribution respectively:

```{=tex}
\begin{align}
p(A,\Sigma) = MNIW(\underline{A},\underline{S},\underline{V},\underline{\nu})\\
A|\Sigma \sim MN_{K \times N}(\underline{A},\Sigma,\underline{V}) \\
\Sigma \sim IW_N(\underline{S},\underline{\nu}),
\end{align}
```

with the priors to be specified as Minnesota priors following @doan1984forecasting

```{=tex}
\begin{align}
\underline{A} = [0_{N \times 1}, \quad I_N, \quad 0_{N \times (p-1)N}]'\\
\underline{V} = diag([\kappa_2, \quad \kappa_1 (p^{-2} \otimes I_N)]).
\end{align}
```

```{r Minnesota priors, echo=TRUE}
N = ncol(Y.bv)
    
#set kapps
kappa.1 <- 1
kappa.2 <-100
K = 1 + (p*N)
  
A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N+1),] = diag(N)
V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior     = diag(diag(Sigma.hat))
nu.prior    = N+1

round(A.prior,3)
V.prior
round(S.prior,3)
round(nu.prior,3)
```

The joint posterior distribution is then given by the product of the likelihood and the priors:

```{=tex}
\begin{align}
p(A,\Sigma|Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \\
&\times exp(-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)'(Y-XA)]) \\
&\times \det(\Sigma)^{-\frac{N+K+\underline{v}+1}{2}} \\
&\times exp(-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A})'\underline(V)^{-1}(A-\underline{A})]) \\
&\times exp(-\frac{1}{2}tr[\Sigma^{-1}\underline{S}]).
\end{align}
```

Combining terms yields the following distributions for $A$ and $\Sigma$:

```{=tex}
\begin{align}
p(A|Y,X,\Sigma) = MN_{K \times N}(\bar{A}, \Sigma, \bar{V}) \\
p(\Sigma|Y,X) = IW_N(\bar{S},\bar{\nu}),
\end{align}
```

where parameters $\bar{A}$ , $\bar{V}$, $\bar{S}$ and $\bar{\nu}$ characterising the posterior distribution and are given by

```{=tex}
\begin{align}
\bar{V} &= (X'X + \underline{V}^{-1})^{-1} \\
\bar{A} &= \bar{V}(X'Y + \underline{V}^{-1}\underline{A}) \\
\bar{\nu} &= T + \underline{\nu} \\
\bar{S} &= \underline{S} + Y'Y + \underline{A}'\underline{V}^{-1}\underline{A} - \bar{A}'\bar{V}^{-1}\bar{A}.
\end{align}
```

```{r NIW posterior paramters, echo=TRUE}
V.bar.inv   = t(X.bv)%*%X.bv + diag(1/diag(V.prior))
V.bar       = solve(V.bar.inv)
A.bar       = V.bar%*%(t(X.bv)%*%Y.bv + diag(1/diag(V.prior))%*%A.prior)
nu.bar      = nrow(Y.bv) + nu.prior
S.bar       = S.prior + t(Y.bv)%*%Y.bv + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
S.bar.inv   = solve(S.bar)
```

### Posterior draws and Gibbs sampler

```{r Posterior darws , echo=TRUE, message=FALSE, warning=FALSE}
Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
Sigma.posterior   = apply(Sigma.posterior,3,solve)
Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
L                 = t(chol(V.bar))
for (s in 1:S){
  A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
}


A.E         = apply(A.posterior,1:2,mean)
A.sd        = apply(A.posterior,1:2,sd)
Sigma.E     = apply(Sigma.posterior,1:2,mean)
Sigma.sd    = apply(Sigma.posterior,1:2,sd)

round(A.E, 3)
```

```{r echo=TRUE}
round(Sigma.E, 3)
```

The estimated parameters are close to the expected values of 1 for a 1-period memory of a random process.

```{r Posterior function, echo=TRUE}
#| fold: true

posterior <- function(Y, X, p, S){
  N = ncol(Y)
  #S = 100
  
  #set kappas
  kappa.1   = 1
  kappa.2   = 100
  K = 1 + (p*N)
  
  #minnesota priors  
  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
  A.prior[2:(N+1),] = diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  
  #posterior draws
  V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))
  V.bar       = solve(V.bar.inv)
  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
  nu.bar      = nrow(Y) + nu.prior
  S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv   = solve(S.bar)
 
  Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
  Sigma.posterior   = apply(Sigma.posterior,3,solve)
  Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
  A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
  L                 = t(chol(V.bar))
  for (s in 1:S){
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
  }
  list(A.posterior = A.posterior, Sigma.posterior = Sigma.posterior)
}
```

# Model extension: T-Distributed errors

An interesting specification in our model of forecasting real wages is the introduction of t-Distributed error terms. This specification is particularly notable because the t-distribution, with its heavier tails, may better account for potential outliers and the distributional characteristics of the time series data. This approach aims to enhance the model's robustness and accuracy in capturing the variability in real wage changes.

Following @chan2020large, the general Covariance structure is

$$
vec(E) \sim \operatorname{N}(0, \Sigma \otimes \Omega)
$$

where $\Omega$ is a $T \times T$ covariance matrix.

If $\Omega=diag(\lambda_1,...,\lambda_T)$ and each $\lambda_t \mid \nu \sim \operatorname{IG2}(\nu, \nu)$, then $\epsilon_t$ has a multivariate t-distribution.

The likelihood is now given by

```{=tex}
\begin{align}
p(Y|A,\Sigma, \Omega) &= (2\pi)^{-\frac{TN}{2}} \det(\Omega)^{-\frac{T}{2}} det(\Sigma)^{-\frac{T}{2}} exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\hat{A})'X'X(A-\hat{A}) \right] \right\}\\
&\times exp \left\{-\frac{1}{2} tr \left[\Sigma^{-1}(Y-X \hat{A})'(Y-X \hat{A}) \right] \right\}.
\end{align}
```

In this framework, $A$ and $\Sigma$ will follow a normal-inverse-Wishart prior:

$$
\Sigma \sim \operatorname{IW}(\underline{s}, \underline{v}), \\
\quad (\operatorname{vec}(A) \mid \Sigma) \sim \mathcal{N}(\operatorname{vec}(\underline{A}), \Sigma \otimes \underline{V})
$$

with joint density

```{=tex}
\begin{align}
p(A, \Sigma) &= \det(\Sigma)^{-\frac{\nu +N+K}{2}} \times exp\{-\frac{1}{2} tr[\Sigma^{-1}\underline{s}]\}  \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A})'\underline{V}^{-1}(A-\underline{A})]\}
\end{align}
```

The kernel of the joint posterior as our product of Likelihood and joint density is then expressed as

```{=tex}
\begin{align}
p(A,\Sigma|Y, \Omega) &\propto \det(\Sigma)^{-\frac{T}{2}} \times \det(\Omega)^{-\frac{N}{2}} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)' \Omega^{-1} (Y-XA)]\} \\
&\times \det(\Sigma)^{-\frac{N+K+\underline{v}+1}{2}} \times \det(\Sigma)^{-\frac{\nu +N+K}{2}}\\ 
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}\underline{s}]\}
\end{align}
```

After rearranging terms, the Kernel of the posterior distribution can be obtained as

```{=tex}
\begin{align}
&= \det(\Sigma)^{-\frac{N+K+\bar{v}+1}{2}} \times \det(\Omega)^{-\frac{N}{2}}\\
&\times exp\{-\frac{1}{2} tr[\Sigma^{-1}(A-\bar{A})'\bar{V}^{-1}(A-\bar{A})]\}\\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}\bar{s}]\},
\end{align}
```
which follows a multinomial-normal-inverse-wishart distribution:

$$
p(A, \Sigma|Y,X) \sim \operatorname{MNIW}(\bar{A}, \bar{\nu}, \bar{s}, \bar{v}).
$$

The posterior parameters are then given as follows:

```{=tex}
\begin{align}
\bar{V} &= (X'\Omega^{-1}X + \underline{V}^{-1})^{-1} \\
\bar{A} &= \bar{V}(X'\Omega^{-1}Y + \underline{V}^{-1}\underline{A}) \\
\bar{\nu} &= T + \underline{\nu} \\
\bar{S} &= \underline{S} + Y'\Omega^{-1}Y + \underline{A}'\underline{V}^{-1}\underline{A} - \bar{A}'\bar{V}^{-1}\bar{A}.
\end{align}
```

### Distribution of $\lambda_t$

The likelihood defines as

$$
L(A, \Sigma, \lambda | Y, X) = \lambda_t^{-\frac{N}{2}} \times \exp\left\{-\frac{1}{2} \frac{1}{\lambda_t} \epsilon_t' \Sigma^{-1} \epsilon_t \right\}.
$$

The prior density of $\lambda \sim \operatorname{IG2}(\nu_{\lambda}, \nu_{\lambda})$ defined as

$$
p(\lambda_t|\nu_{\lambda}) = \lambda_t^{ -\frac{N+ \nu_{\lambda} +2}{2} } \times exp\{-\frac{1}{2} \frac{1}{\lambda_t} \nu_{\lambda}\}.
$$


The joint posterior now derives in the common procedure

```{=tex}
\begin{align}
p(\lambda_t|Y,X,A,\Sigma) &\propto L(A,\Sigma,\lambda|Y,X) \times p(\lambda_t|\nu_{\lambda})\\
&= \lambda_t^{-\frac{N+ \nu_{\lambda} +2}{2}} \times exp\{-\frac{1}{2} \frac{1}{\lambda_t}(\nu_{\lambda} + \epsilon_t' \Sigma^{-1} \epsilon_t),
\end{align}
```

with $\epsilon_t = (y_t-x_t'A)$.

The kernel of the joint posterior follows an inverse gamma 2 distribution.

$$
p(\lambda_t|Y,X,A,\Sigma) \sim \operatorname{IG2}(N+\nu_{\lambda}, \nu_{\lambda}+\epsilon_t'\Sigma^{-1}\epsilon_t)
$$

Sampling algorithm:

1. Draw $\Sigma^{(s)}$ from $\operatorname{IW_N}(\bar{S},\bar{\nu})$.

2. Draw $A^{(s)}$ from $\operatorname{MN_{K\times N}}(\bar{A},\Sigma^{(s)},\bar{V})$.

3. Draw $\lambda_t^{(s)}$ with $T$ $\lambda_t$ from $\operatorname{IG2} (\bar\nu_{\lambda},\bar{s}_{\lambda})$.

(Extension

4. Draw ${\nu_{\lambda}^*}^{(s)}$ from $\operatorname{N}(\nu^{s-1}, var_{\nu})$ applying the Metropolis-Hastings algorithm.)

```{r Posterior extension, echo=TRUE}
posterior_t <- function(Y, X, p, S){
  
  #p=1
  #S=1000
  
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  
  N = ncol(Y)
  t <- NROW(Y)
  kappa.1   = 1
  kappa.2   = 100
  K = 1 + (p*N)
  
  A.prior     = matrix(0, K , N)
  A.prior[2:(N+1),] = diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  lambda.nu.prior = 5
  
  
  
  lambda.0 = rinvgamma(t, lambda.nu.prior/2, lambda.nu.prior/2)
  
  
  Sigma.posterior.draws <- array(dim = c(N, N, S))
  A.posterior.draws <- array(dim = c(K, N, S))
  lambda.posterior.draws <- array(NA,c(t,S))
  
  
  for (s in 1:S){
    if (s == 1) {
      lambda.s = lambda.0
    } else {
      lambda.s    = lambda.posterior.draws[,s-1]
    }
    Omega = (diag(lambda.s))
    Omega.inv = diag(1/lambda.s)
    
    V.bar.ext       = solve(t(X)%*% Omega.inv%*%X + solve(V.prior))
    A.bar.ext       = V.bar.ext%*%(t(X)%*% Omega.inv%*%Y + solve(V.prior)%*%A.prior)
    nu.bar.ext      = t + nu.prior
    S.bar.ext       = S.prior + t(Y)%*% Omega.inv%*%Y + t(A.prior)%*%solve(V.prior)%*%A.prior - t(A.bar.ext)%*%solve(V.bar.ext)%*%A.bar.ext
    S.bar.ext.inv   = solve(S.bar.ext)
    
    Sigma.inv.draw = rWishart(1, nu.bar.ext, S.bar.ext.inv)[,,1]
    Sigma.posterior.draws[,,s] = solve(Sigma.inv.draw)
    A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.bar.ext), sigma = Sigma.posterior.draws[,,s] %x% V.bar.ext), ncol=N)
    
    for (x in 1:t){
    u_t <- Y[x,] - t(X[x,]) %*% A.posterior.draws[,,s]
    lambda = rinvgamma(1, N+lambda.nu.prior, lambda.nu.prior + u_t %*% Sigma.posterior.draws[,,s] %*% t(u_t) )
    lambda.posterior.draws[x,s] <- lambda
    }
    
  }  
  pris = (list(A.posterior.draws = A.posterior.draws, 
               Sigma.posterior.draws = Sigma.posterior.draws,
               lambda.posterior.draws=lambda.posterior.draws))

}

```

```{r stats}
stats = posterior_t(Y.bv, X.bv, 1, 1000)

round(apply(stats[[1]][,,s], 1:2, mean),2)

round(apply(stats[[2]][,,s], 1:2, mean),2)

round(mean(stats[[3]][,1:1000]),2)

round(apply(stats[[2]][,,s], 1:2, mean),2) * round(mean(stats[[3]][,1:1000]),2)

```

One can observe, that the means represent the expected values, whereas the variances lie a bit further apart from 1.

#### Include $\nu$ sampling

A possible further extension of the model is the implementation of a sampling algorithm for $\nu$. This can be applied using a Metropolis-Hastings structure.  

```{r nu_sampling, echo=TRUE }
#1st version
library(MCMCpack)
library(mvtnorm)

set.seed(678)

S = 100
# Estimate initial parameters

rw_data = data.frame(matrix(nrow=100, ncol=2))
rw_data[,1] = cumsum(rnorm(100,0,1))
rw_data[,2] = cumsum(rnorm(100,0,1))


y.bv <- ts(rw_data,  frequency=1)
N = ncol(y.bv)
p       = 1
h       = 8
Y.bv       = ts(y.bv[(p+1):nrow(y.bv),])
X.bv      = matrix(1,nrow(Y.bv),1)
for (i in 1:p){
  X.bv     = cbind(X.bv,y.bv[(p+1):nrow(y.bv)-i,])
}

A.hat = solve(t(X.bv) %*% X.bv) %*% t(X.bv) %*% Y.bv
Sigma.hat = t(Y.bv - X.bv %*% A.hat) %*% (Y.bv - X.bv %*% A.hat) / nrow(Y.bv)

t <- NROW(Y.bv)
kappa.1 <- 0.02^2
kappa.2 <- 100
K = 1 + (p * N)

A.prior = matrix(0, K, N)
A.prior[2:(N + 1),] = diag(N)
V.prior = diag(c(kappa.2, kappa.1 * ((1:p)^(-2)) %x% rep(1, N)))
S.prior = diag(diag(Sigma.hat))
nu.prior = N + 1
lambda.nu.prior = 5

# Initialize lambda
lambda.0 = rinvgamma(t, lambda.nu.prior / 2, lambda.nu.prior / 2)

# Initialize arrays for posterior draws
Sigma.posterior.draws <- array(dim = c(N, N, S))
A.posterior.draws <- array(dim = c(K, N, S))
lambda.posterior.draws <- array(NA, c(t, S))
lambda.nu <- numeric(S)
alpha <- numeric(S)
ratio <- numeric(S)

var.lambda.nu <- 1


for (s in 1:S) {
  if (s == 1) {
    lambda.s <- lambda.0
    lambda.nu[s] <- lambda.nu.prior
  } else {
    lambda.s <- lambda.posterior.draws[, s - 1]
    lambda.nu[s] <- lambda.nu[s-1]
  }
  
  Omega <- diag(lambda.s)
  Omega.inv <- diag(1 / lambda.s)
  
  V.bar.ext = solve(t(X.bv) %*% Omega.inv %*% X.bv + solve(V.prior))
  A.bar.ext = V.bar.ext %*% (t(X.bv) %*% Omega.inv %*% Y.bv + solve(V.prior) %*% A.prior)
  nu.bar.ext = t + nu.prior
  S.bar.ext = S.prior + t(Y.bv) %*% Omega.inv %*% Y.bv + t(A.prior) %*% solve(V.prior) %*% A.prior - t(A.bar.ext) %*% solve(V.bar.ext) %*% A.bar.ext
  S.bar.ext.inv = solve(S.bar.ext)
  
  Sigma.inv.draw = rWishart(1, nu.bar.ext, S.bar.ext.inv)[,,1]
  Sigma.posterior.draws[,,s] = solve(Sigma.inv.draw)
  A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean = as.vector(A.bar.ext), sigma = Sigma.posterior.draws[,,s] %x% V.bar.ext), ncol = N)
  
  for (x in 1:t) {
    u_t <- Y.bv[x,] - t(X.bv[x,]) %*% A.posterior.draws[,,s]
    lambda.posterior.draws[x, s] <- rinvgamma(1, N + lambda.nu[s], lambda.nu[s] + u_t %*% Sigma.posterior.draws[,,s] %*% t(u_t))
  }
  
  if (s>=2){
    
    
    lambda.nu.star <- rnorm(1, mean = lambda.nu[s-1], sd = var.lambda.nu)
    
    log_likelihood_star <- sum(dgamma(lambda.posterior.draws[,s], shape = lambda.nu.star, rate = lambda.nu.star, log = TRUE))
    log_prior_star <- dnorm(lambda.nu.star, mean = lambda.nu[s-1], sd = var.lambda.nu, log = TRUE)
    
    log_likelihood_curr <- sum(dgamma(lambda.posterior.draws[,s], shape = lambda.nu[s-1], rate = lambda.nu[s-1], log = TRUE))
    log_prior_curr <- dnorm(lambda.nu[s-1], mean = lambda.nu[s-1], sd = var.lambda.nu, log = TRUE)
    
    log_ratio <- (log_likelihood_star + log_prior_star) - (log_likelihood_curr + log_prior_curr)
    
    ratio[s] <- exp(log_ratio)
    
    alpha[s] <- min(1, ratio[s])
    
    if (runif(1) < alpha[s]) {
      lambda.nu[s] <- lambda.nu.star
    } else{
      lambda.nu[s] <- lambda.nu[s-1]
    }
  }
}
acceptance_ratio <- sum(ratio[ratio < 1])

print(acceptance_ratio)

# Plot the nu draws
plot(lambda.nu, type = 'l', col = 'blue', lwd = 2, 
     main = expression(paste(lambda_nu, " draws")), 
     xlab = "Iteration", ylab = expression(lambda_nu))
```

## Stochastic volatility

As a further extension, stochastic volatility can be added to the model.
Therefor, conditional heteroskedasticity will be defined as:

$$
E|X \sim\mathcal{MN}_{T\times N}\left(0_{T\times N},\Sigma,\text{diag}\left(\sigma^{2}\right)\right)
$$

```{r}




```



# Real Wage data forecasts

```{r Model set-up  , echo=FALSE, results='hide'}
y <- ts(df, start=c(1998, 4), frequency=4)
#N = ncol(y)
p       = 4
#K       = 1 + p*N
#S       = 100
#h       = 8
Y       = ts(y[(p+1):nrow(y),])
X       = matrix(1,nrow(Y),1)
for (i in 1:p){
  X     = cbind(X,y[(p+1):nrow(y)-i,])
}

h       = 8
S       = 1000
N = ncol(Y)
K = 1 + (p*N)
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

```


```{r forecast base functions, echo=TRUE}
forecasts_base <- function(Y, A.posterior, Sigma.posterior, h, S, p){
  N = ncol(Y)
  Y.h         = array(NA,c(h,N,S))
  for (s in 1:S){
    x.Ti        = Y[(nrow(Y)-p+1):nrow(Y),]
    x.Ti        = x.Ti[p:1,] 
  

    for (i in 1:h){
      x.T         = c(1,as.vector(t(x.Ti)))
      Y.h[i,,s]   = rmvnorm(1, mean = x.T%*%A.posterior[,,s], sigma=Sigma.posterior[,,s])
      
      if (p==1){
        x.Ti        = Y.h[i,,s]
      } else {
        x.Ti        = rbind(Y.h[i,,s],x.Ti[1:(p-1),])
      }
    }
  }
  
  list(Y.h = Y.h)
}

```

```{r forecast t, echo=TRUE}
forecasts_t <- function(Y, A.posterior.draws, Sigma.posterior.draws, lambda.posterior.draws, h, S, p){
  N = ncol(Y)
  t= nrow(Y)
  Y.h         = array(NA,c(h,N,S))
  for (s in 1:S){
    x.Ti        = Y[(nrow(Y)-p+1):nrow(Y),]
    x.Ti        = x.Ti[p:1,] 
  

    for (i in 1:h){
      x.T         = c(1,as.vector(t(x.Ti)))
      Y.h[i,,s]   = rmvnorm(1, mean = x.T%*%A.posterior.draws[,,s], sigma= Sigma.posterior.draws[,,s] %*% (lambda.posterior.draws[,s] %*% diag(t))) 
      
      if (p==1){
        x.Ti        = Y.h[i,,s]
      } else {
        x.Ti        = rbind(Y.h[i,,s],x.Ti[1:(p-1),])
      }
    }
  }
  
  list(Y.h = Y.h)
}

```


```{r Call functions, echo=TRUE}
post.results <- posterior(Y, X, 4, 1000)

forecasts_norm <- forecasts_base(Y, post.results$A.posterior, post.results$Sigma.posterior, 8, 1000, 4)

post.t.results <- posterior_t(Y, X, 4, 1000)

forecasts_t <- forecasts_base(Y, post.t.results$A.posterior.draws, post.t.results$Sigma.posterior.draws, 8, 1000, 4)
#forecasts_t <- forecasts(Y, post.t.results$A.posterior.draws, post.t.results$Sigma.posterior.draws, post.t.results$lambda.posterior.draws, 1000, 4)
```

```{r forecast plots}
mcxs5_shade <- rgb(240/255, 128/255, 128/255, alpha = 0.4)
par(mfrow = c(5, 2), mar = c(2, 2, 1, 1), oma = c(0, 0, 2, 0))
par(mfrow = c(5, 2), mar = c(2, 2, 1, 1), oma = c(0, 0, 2, 0))
x_dates <- format(as.yearqtr(c(dates, seq(tail(dates, 1), by = "quarter", length.out = h + 1)[-1])), "%Y Q%q")


quarters <- format(as.yearqtr(x_dates), "%q")
years <- format(as.yearqtr(x_dates), "%Y")

unique_years <- unique(years)
year_positions <- sapply(unique_years, function(y) mean(which(years == y)))

forecast.plots <- function(Y.h1, Y.h2, y){

  for (x in 1:9) {
    point.f1 <- apply(Y.h1[, x, ], 1, mean)
    interval.f1 <- apply(Y.h1[, x, ], 1, hdi, credMass = 0.68)
    
    point.f2 <- apply(Y.h2[, x, ], 1, mean)
    interval.f2 <- apply(Y.h2[, x, ], 1, hdi, credMass = 0.68)
    
    combined_data1 <- c(y[, x], point.f1)
    range_val1 <- range(combined_data1, interval.f1)

    plot(1:(nrow(y) + h), combined_data1, type = "l", ylim = range_val1, xlab = "", ylab = "", col = mcxs2, lwd = 2, main = paste("Forecast", var_names[x]),  bty = "n", xaxt = "n")
    lines((nrow(y) + 1):(nrow(y) + h), point.f2, col = 'red', lwd = 2)
    #axis(1, at = 1:(nrow(y) + h), cex.axis = 0.7, tck = 0)
    axis(1, at = 1:(nrow(y) + h), labels = quarters, tick = TRUE, las = 2, cex.axis = 0.5)
    axis(1, at = year_positions, labels = unique_years, tick = FALSE, line = -0.3, cex.axis = 0.4)
    
    
    abline(v = nrow(y)+1, col = mcxs1)  
    polygon(c((nrow(y)+1):(nrow(y)+h), rev((nrow(y)+1):(nrow(y)+h))),
            c(interval.f2[1,], rev(interval.f2[2,])), col = mcxs5_shade, border =mcxs5_shade)
    polygon(c((nrow(y)+1):(nrow(y)+h), rev((nrow(y)+1):(nrow(y)+h))),
            c(interval.f1[1,], rev(interval.f1[2,])), col = mcxs2.shade1, border = mcxs2.shade1)
    legend("topleft", legend = c("Base", "T-Distr"), col = c(mcxs2, "red"), lwd = 2, cex = 0.6)
  }
}

forecast.plots(forecasts_norm$Y.h, forecasts_t$Y.h, y)
```

We observe the real wages to stabilize in the future, but not to reach levels of the pre-Covid area.
The mean predictions for normal distributed errors and t-distributed errors mostly align, whereas the intervals of the t-distributed model show a wider spread. 

### Real wages forecast

```{r single forecast}
par(mfrow = c(1, 1), mar = c(4, 4, 2, 1), oma = c(0, 0, 2, 0))

single.forecast.plot <- function(forecasts_norm, forecasts_t, y){

    point.f1.wages = apply(forecasts_norm$Y.h[,1,],1,mean)
    interval.f1.wages = apply(forecasts_norm$Y.h[,1,],1,hdi,credMass=0.68)
    
    point.f2.wages <- apply(forecasts_t$Y.h[, 1, ], 1, mean)
    interval.f2.wages <- apply(forecasts_t$Y.h[, 1, ], 1, hdi, credMass = 0.68)
    
    combined_data1.wages <- c(y[, 1], point.f1.wages)
    range_val1.wages <- range(combined_data1.wages, interval.f1.wages)
    
    plot(1:(nrow(y) + h), combined_data1.wages, type = "l", ylim = range_val1.wages, 
         xlab = "", ylab = "", col = mcxs2, lwd = 2, 
         main = paste("Forecast", var_names[1]), bty = "n", xaxt = "n")
    
    lines((nrow(y) + 1):(nrow(y) + h), point.f2.wages, col = 'red', lwd = 2)
    axis(1, at = 1:(nrow(y) + h), labels = quarters, tick = TRUE, las = 2, cex.axis = 0.23)
    axis(1, at = year_positions, labels = unique_years, tick = FALSE, line = -0.1, cex.axis = 0.4)
    
    abline(v = nrow(y) + 1, col = mcxs1)
    polygon(c((nrow(y) + 1):(nrow(y) + h), rev((nrow(y) + 1):(nrow(y) + h))),
            c(interval.f2.wages[1, ], rev(interval.f2.wages[2, ])), col = mcxs5_shade, border = mcxs5_shade)
    polygon(c((nrow(y) + 1):(nrow(y) + h), rev((nrow(y) + 1):(nrow(y) + h))),
            c(interval.f1.wages[1, ], rev(interval.f1.wages[2, ])), col = mcxs2.shade1, border = mcxs2.shade1)
    legend("topleft", legend = c("Base", "T-Distr"), col = c(mcxs2, "red"), lwd = 2, cex = 0.6)
}

single.forecast.plot(forecasts_norm, forecasts_t, y)
```

### Forecast horizons

```{r forecast horizons}
plot_predictive_densities <- function(Y.h, distribution_name) {
  h <- dim(Y.h)[1]  # Forecast horizon
  limits.1 <- range(Y.h[, 1, ])
  point.f <- apply(Y.h[, 1, ], 1, mean)
  interval.f <- apply(Y.h[, 1, ], 1, hdi, credMass = 0.90)
  
  x <- seq(from = limits.1[1], to = limits.1[2], length.out = 100)
  z <- matrix(NA, nrow = h, ncol = length(x) - 1)
  for (i in 1:h) {
    z[i, ] <- hist(Y.h[i, 1, ], breaks = x, plot = FALSE)$density
  }
  x <- hist(Y.h[h, 1, ], breaks = x, plot = FALSE)$mids
  yy <- 1:h
  z <- t(z)
  
  theta <- 180
  phi <- 15.5
  f4 <- persp3D(x = x, y = yy, z = z, phi = phi, theta = theta, 
                xlab = "\nReal Wages[t+h|t]", ylab = "h", zlab = "\nPredictive Densities", 
                shade = NA, border = NA, ticktype = "detailed", nticks = 3, cex.lab = 1, col = NA, plot = FALSE)
  
  perspbox(x = x, y = yy, z = z, bty = "f", col.axis = "black", phi = phi, theta = theta, 
           xlab = "\nReal Wages[t+h|t]", ylab = "h", zlab = "\nPredictive Densities", 
           ticktype = "detailed", nticks = 3, cex.lab = 1, col = NULL, plot = TRUE)
  
  polygon3D(x = c(interval.f[1, ], interval.f[2, h:1]), y = c(1:h, h:1), z = rep(0, 2 * h), 
            col = mcxs2.shade1, NAcol = "white", border = NA, add = TRUE, plot = TRUE)
  
  for (i in 1:h) {
    f4.l <- trans3d(x = x, y = yy[i], z = z[, i], pmat = f4)
    lines(f4.l, lwd = 0.5, col = "black")
  }
  
  f4.l1 <- trans3d(x = point.f, y = yy, z = 0, pmat = f4)
  lines(f4.l1, lwd = 2, col = mcxs2)
  title(main = paste(distribution_name, "Distribution"))
}

# Set up the plotting window
par(mfrow = c(1, 2), mar = c(2, 2, 1, 1), oma = c(0, 0, 2, 0))

# Plot for normal distribution
plot_predictive_densities(forecasts_norm$Y.h, "Normal")
plot_predictive_densities(forecasts_t$Y.h, "t-Distribution")
```

### Joint predictive density plots

```{r predictive density}
Y.h_norm <- forecasts_norm$Y.h  
Y.h_t <- forecasts_t$Y.h  

limits.i1_norm <- range(Y.h_norm[1, 1, ])
limits.i2_norm <- range(Y.h_norm[2, 1, ])

limits.i1_t <- range(Y.h_t[1, 1, ])
limits.i2_t <- range(Y.h_t[2, 1, ])

bands <- 100

predictive.kernel_norm <- kde2d(x = Y.h_norm[1, 1, ], y = Y.h_norm[2, 1, ], n = bands, lims = c(limits.i1_norm, limits.i2_norm))

predictive.kernel_t <- kde2d(x = Y.h_t[1, 1, ], y = Y.h_t[2, 1, ], n = bands, lims = c(limits.i1_t, limits.i2_t))

marginal.i1_norm <- apply(predictive.kernel_norm$z, 1, sum)
marginal.i1_norm <- max(predictive.kernel_norm$z) * marginal.i1_norm / max(marginal.i1_norm)

marginal.i2_norm <- apply(predictive.kernel_norm$z, 2, sum)
marginal.i2_norm <- max(predictive.kernel_norm$z) * marginal.i2_norm / max(marginal.i2_norm)

marginal.i1_t <- apply(predictive.kernel_t$z, 1, sum)
marginal.i1_t <- max(predictive.kernel_t$z) * marginal.i1_t / max(marginal.i1_t)

marginal.i2_t <- apply(predictive.kernel_t$z, 2, sum)
marginal.i2_t <- max(predictive.kernel_t$z) * marginal.i2_t / max(marginal.i2_t)

par(mfrow = c(1, 2), mar = c(2, 2, 1, 1), oma = c(0, 0, 2, 0))

f1_norm <- persp3D(x = predictive.kernel_norm$x, y = predictive.kernel_norm$y, z = predictive.kernel_norm$z, 
                   phi = 25, theta = 25, xlab = "\nReal Wages[t+1|t]", ylab = "\nReal Wages[t+2|t]", 
                   zlab = "\nPredictive Density Real Wages h=1:2", shade = 0, border = NA, 
                   ticktype = "detailed", nticks = 2, cex.lab = 1, col = "white", main = "Normal Distribution")

f1.l1_norm <- trans3d(x = predictive.kernel_norm$x, y = rep(max(predictive.kernel_norm$y), length(predictive.kernel_norm$x)), z = marginal.i1_norm, pmat = f1_norm)
lines(f1.l1_norm, lwd = 2, col = mcxs2)

f1.l2_norm <- trans3d(x = rep(min(predictive.kernel_norm$x), length(predictive.kernel_norm$y)), y = predictive.kernel_norm$y, z = marginal.i2_norm, pmat = f1_norm)
lines(f1.l2_norm, lwd = 2, col = mcxs1)

persp3D(x = predictive.kernel_norm$x, y = predictive.kernel_norm$y, z = predictive.kernel_norm$z, 
        shade = .5, border = NA, col = mcxs2.shade1, add = TRUE)

# Plot for t-distribution
f1_t <- persp3D(x = predictive.kernel_t$x, y = predictive.kernel_t$y, z = predictive.kernel_t$z, 
                phi = 25, theta = 25, xlab = "\nReal Wages[t+1|t]", ylab = "\nReal Wages[t+2|t]", 
                zlab = "\nPredictive Density Real Wages h=1:2", shade = 0, border = NA, 
                ticktype = "detailed", nticks = 2, cex.lab = 1, col = "white", main = "t-Distribution")

f1.l1_t <- trans3d(x = predictive.kernel_t$x, y = rep(max(predictive.kernel_t$y), length(predictive.kernel_t$x)), z = marginal.i1_t, pmat = f1_t)
lines(f1.l1_t, lwd = 2, col = mcxs2)

f1.l2_t <- trans3d(x = rep(min(predictive.kernel_t$x), length(predictive.kernel_t$y)), y = predictive.kernel_t$y, z = marginal.i2_t, pmat = f1_t)
lines(f1.l2_t, lwd = 2, col = mcxs1)

persp3D(x = predictive.kernel_t$x, y = predictive.kernel_t$y, z = predictive.kernel_t$z, 
        shade = .5, border = NA, col = mcxs2.shade1, add = TRUE)
```


### Forecast table for wage data 

```{r Forecast table}
forecast.table2 <- function(Y.h1, Y.h2, y) {
  forecast_table2 <- data.frame(
    Period = integer(),
    Baseline_Forecast = numeric(),
    Extension_Forecast = numeric(),
    Baseline_Percentage_Change = numeric(),
    Extension_Percentage_Change = numeric(),
    stringsAsFactors = FALSE
  )
  
  n_periods <- dim(Y.h1)[1]
  
  point.f1.table2 <- apply(Y.h1[, 1, ], 1, mean)
  point.f2.table2 <- apply(Y.h2[, 1, ], 1, mean)
  
  # Include period 0 (last observation from the data)
  actual_value_0 <- y[nrow(y), 1]
  forecast_table2 <- rbind(forecast_table2, data.frame(
    Period = 0,
    Base_Forecast = actual_value_0,
    Ext_Forecast = actual_value_0,
    Base_Perc_Change = 0,
    Ext_Perc_Change = 0
  ))
  
  for (period in 1:n_periods) {
    if (period == 1) {
      previous_baseline_value <- actual_value_0
      previous_extension_value <- actual_value_0
    } else {
      previous_baseline_value <- point.f1.table2[period - 1]
      previous_extension_value <- point.f2.table2[period - 1]
    }
    
    baseline_pct_change <- ((exp(point.f1.table2[period]) - exp(previous_baseline_value)) / exp(previous_baseline_value)) * 100
    extension_pct_change <- ((exp(point.f2.table2[period]) - exp(previous_extension_value)) / exp(previous_extension_value)) * 100
    
    forecast_table2 <- rbind(forecast_table2, data.frame(
      Period = period,
      Base_Forecast = point.f1.table2[period],
      Ext_Forecast = point.f2.table2[period],
      Base_Perc_Change = round(baseline_pct_change, 2),
      Ext_Perc_Change = round(extension_pct_change, 2)
    ))
  }
  
  return(forecast_table2)
}

forecast_table_out <- forecast.table2(forecasts_norm$Y.h, forecasts_t$Y.h, y)
rownames(forecast_table_out) <- NULL
print(forecast_table_out)

```
# References {.unnumbered}
